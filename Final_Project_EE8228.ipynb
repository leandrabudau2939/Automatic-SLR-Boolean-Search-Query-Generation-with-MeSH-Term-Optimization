{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "fSd0tGP515Jl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIvF8IGOQRo8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.22\n",
        "!pip install sacremoses\n",
        "!pip install biopython\n",
        "!pip install sentence-transformers\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERTMeSH**"
      ],
      "metadata": {
        "id": "SGBdBKfuvS5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer_BERTMeSH = AutoTokenizer.from_pretrained(\"osanseviero/test_model_bertmesh\")\n",
        "model_BERTMeSH = AutoModel.from_pretrained(\"osanseviero/test_model_bertmesh\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "TD-PNEXkueaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Documents from Pubmed**"
      ],
      "metadata": {
        "id": "2PRJbi-QoIH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "def search_pubmed(query, num_results=5):\n",
        "    Entrez.email = \"leandra.budau@torontomu.ca\"\n",
        "\n",
        "    # Search query in Pubmed database\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=num_results)\n",
        "    record = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    # Retrieve the list of PubMed IDs (PMID)\n",
        "    pmids = record[\"IdList\"]\n",
        "\n",
        "    return pmids"
      ],
      "metadata": {
        "id": "EftNUELtoCjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Title from Article based on PMID**"
      ],
      "metadata": {
        "id": "bpsJzqBDtYX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pubmed_data(pmid):\n",
        "    Entrez.email = \"leandra.budau@torontomu.ca\"\n",
        "\n",
        "    # Search for document in Pubmed Database\n",
        "    try:\n",
        "        handle = Entrez.efetch(db=\"pubmed\", id=pmid, retmode=\"xml\")\n",
        "        record = Entrez.read(handle)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Get title and abstract from pubmed article\n",
        "    try:\n",
        "      articles = record['PubmedArticle']\n",
        "\n",
        "    # Check if there is at least one article\n",
        "      if articles:\n",
        "          medline_citation = articles[0].get('MedlineCitation', {})\n",
        "          article = medline_citation.get('Article', {})\n",
        "          title = article.get('ArticleTitle', 'Title not available')\n",
        "      else:\n",
        "          title = ''\n",
        "\n",
        "    except (KeyError, IndexError) as e:\n",
        "      print(f\"Error: {e}\")\n",
        "      print(\"Could not retrieve article information.\")\n",
        "\n",
        "    return title"
      ],
      "metadata": {
        "id": "6rWop9BwqD_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sending Prompts to ChatGPT**"
      ],
      "metadata": {
        "id": "XRwf0I5r8Vyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\",\n",
        ")\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "  messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "  response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "AemiM1dQ8ZtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting Queries by Operators**"
      ],
      "metadata": {
        "id": "q_7JECOTDrjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def split_query_to_frags(query):\n",
        "\n",
        "  fragments = re.split(r'\\bAND\\b', query, flags=re.IGNORECASE)\n",
        "  fragments = [fragment.strip() for fragment in fragments]\n",
        "  return fragments\n",
        "\n",
        "def split_frags_to_terms(fragments):\n",
        "  terms = []\n",
        "\n",
        "  for i in range(len(fragments)):\n",
        "    fragments[i] = fragments[i].replace('(', '')\n",
        "    fragments[i] = fragments[i].replace(')', '')\n",
        "    fragments[i] = fragments[i].replace('\"', '')\n",
        "    fragments[i] = re.split(r'\\bOR\\b', fragments[i], flags=re.IGNORECASE)\n",
        "    fragments[i] = [term.strip() for term in fragments[i]]\n",
        "    terms.append(fragments[i])\n",
        "\n",
        "  return(terms)"
      ],
      "metadata": {
        "id": "ViqlC2OAFjtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get MeSH Term Definitions**"
      ],
      "metadata": {
        "id": "Tok-STnujmTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_mesh_id (mesh_term):\n",
        "  search_url = f'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/esearch.fcgi/' + \\\n",
        "                f'?db=mesh' + \\\n",
        "                f'&term=' + \\\n",
        "                f'' + mesh_term + ''+\\\n",
        "                f'&retmode=json' + \\\n",
        "                f'&sort=relevance' + \\\n",
        "                f'&retmax=5'\n",
        "\n",
        "\n",
        "  link_list = urllib.request.urlopen(search_url).read().decode('utf-8')\n",
        "  summary = json.loads(link_list)\n",
        "  if 'esearchresult' in summary and 'idlist' in summary['esearchresult']:\n",
        "    if len(summary['esearchresult']['idlist']) == 0:\n",
        "      return ''\n",
        "    else:\n",
        "      return (summary['esearchresult']['idlist'][0])\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "def get_mesh_term_description(mesh_term):\n",
        "    mesh_id = get_mesh_id(mesh_term)\n",
        "    url = f\"https://www.ncbi.nlm.nih.gov/mesh/{mesh_id}\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        definition_element = soup.find(class_='mesh_ds_scope_note')\n",
        "        if definition_element:\n",
        "            return definition_element.text.strip()\n",
        "        else:\n",
        "            return \"No definition found for the given MeSH term ID\"\n",
        "    else:\n",
        "        return f\"Error: {response.status_code}\"\n"
      ],
      "metadata": {
        "id": "9X1t3yD-jpcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semantic Similarity**"
      ],
      "metadata": {
        "id": "Ioh5NJBfyoDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def semantic_similarity(sentence1, definitions):\n",
        "  embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "  best_cosine_score = 0\n",
        "  best_definition = 0\n",
        "\n",
        "  for i in range(len(definitions)):\n",
        "    embedding2 = model.encode(definitions[i], convert_to_tensor=True)\n",
        "    cosine_score = util.cos_sim(embedding1, embedding2)\n",
        "    if (cosine_score.item() > best_cosine_score):\n",
        "      best_cosine_score = cosine_score.item()\n",
        "      best_definition = i\n",
        "\n",
        "  return best_definition"
      ],
      "metadata": {
        "id": "eksPzesQynwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read from Test Files and Export Results**"
      ],
      "metadata": {
        "id": "m-i_LrydqacP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def preprocess():\n",
        "  folder_path = '/content/titles/'\n",
        "  topic_numbers = []\n",
        "  titles = []\n",
        "\n",
        "  try:\n",
        "      files_in_folder = os.listdir(folder_path)\n",
        "\n",
        "      for i, filename in enumerate(files_in_folder):\n",
        "          file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "          with open(file_path, 'r', encoding='utf-8') as file:\n",
        "              topic_number = file.readline().strip()\n",
        "              topic_number = topic_number.replace(\"Topic: \", \"\")\n",
        "              file.readline()\n",
        "              title = file.readline().strip()\n",
        "              title = title.replace(\"Title: \", \"\")\n",
        "\n",
        "              topic_numbers.append(topic_number)\n",
        "              titles.append(title)\n",
        "\n",
        "  except FileNotFoundError:\n",
        "      print(f\"Folder not found: {folder_path}\")\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return topic_numbers, titles"
      ],
      "metadata": {
        "id": "ucLly4xqqXwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export Results**"
      ],
      "metadata": {
        "id": "DYcDnFqI13bP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(topic_number, query):\n",
        "  path = \"/content/results/\" + str(topic_number) + \".txt\"\n",
        "  escaped_query = query.replace(\" \", \"%20\")\n",
        "  print (escaped_query)\n",
        "  search_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&sort=relevance&term=' + \\\n",
        "                f'' + escaped_query + '' + \\\n",
        "                f'&retmax=10000'\n",
        "\n",
        "  link_list = urllib.request.urlopen(search_url).read().decode('utf-8')\n",
        "  summary = json.loads(link_list)\n",
        "  pids = summary['esearchresult']['idlist']\n",
        "  print (pids)\n",
        "\n",
        "  with open(path, 'w') as file:\n",
        "    for item in pids:\n",
        "        file.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "SuA7-C5_153x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Put Everything Together**"
      ],
      "metadata": {
        "id": "8oYsusKG82C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mesh_terms = []\n",
        "mesh_definitions = []\n",
        "\n",
        "topic_numbers, title = preprocess()\n",
        "\n",
        "for n in range(len(title)):\n",
        "  # Getting query from ChatGPT based on SLR title\n",
        "  prompt = \"Based on the following SLR title, please provide 5 complex pubmed Entrez formatted query without descriptions, in plain text, such that they may be used directly on Pubmed's website. Please do not include any MeSH terms: \" + title[n]\n",
        "  response = get_completion(prompt)\n",
        "  response = response.splitlines()[0]\n",
        "  if response.startswith('1. '):\n",
        "      response = response[3:]\n",
        "\n",
        "  # Splitting query into fragments\n",
        "  query_fragments = split_query_to_frags(response)\n",
        "\n",
        "  # Splitting fragments into atomic terms\n",
        "  query_terms = split_frags_to_terms(query_fragments)\n",
        "\n",
        "  # Getting MeSH terms from atomic term\n",
        "  for l in range (len(query_terms)):\n",
        "    for k in range (len(query_terms[l])):\n",
        "      pmids = (search_pubmed(query_terms[l][k]))\n",
        "\n",
        "      for i in range(len(pmids)):\n",
        "          result = pubmed_data(int(pmids[i]))\n",
        "          inputs = tokenizer_BERTMeSH([result], padding=\"max_length\")\n",
        "          labels = model_BERTMeSH(**inputs, return_labels=True)\n",
        "          for j in range (len(labels[0])):\n",
        "            if labels[0][j] not in mesh_terms:\n",
        "              mesh_terms.append(labels[0][j])\n",
        "\n",
        "      # Get MeSH term definitions\n",
        "      for m in range (len(mesh_terms)):\n",
        "        if \" \" in mesh_terms[m]:\n",
        "          temp_mesh_term = mesh_terms[m].replace(\" \", \"%20\")\n",
        "        else:\n",
        "          temp_mesh_term = mesh_terms[m]\n",
        "        mesh_definitions.append(get_mesh_term_description(temp_mesh_term))\n",
        "\n",
        "      # Semantic Similarity\n",
        "      best_definition = semantic_similarity(query_terms[l][k], mesh_definitions)\n",
        "      best_mesh_term = mesh_terms[best_definition]\n",
        "\n",
        "      # Combine the free text term and MeSH term\n",
        "      query_terms[l][k] = \"(\\\"\" + query_terms[l][k] + \"\\\"\" + \" OR \" + \"\\\"\" + best_mesh_term + \"\\\"\" + \"[MeSH])\"\n",
        "\n",
        "      # Clear lists\n",
        "      mesh_terms.clear()\n",
        "      mesh_definitions.clear()\n",
        "\n",
        "  # Reformat query into one string\n",
        "  final_query = \" AND \".join([\"(\" + \" OR \".join(row) + \")\" for row in query_terms])\n",
        "\n",
        "  print(title[n])\n",
        "  print(response)\n",
        "  print(final_query)\n",
        "  postprocess(topic_numbers[n], final_query)"
      ],
      "metadata": {
        "id": "ehfXhS5VrWN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}